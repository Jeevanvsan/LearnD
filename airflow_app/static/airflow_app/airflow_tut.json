[
  {
    "title": "Introduction to Airflow",
    "content": "\n        <h3>What is Apache Airflow?</h3>\n        <p>Apache Airflow is an open-source platform for programmatically authoring, scheduling, and monitoring workflows. It allows you to define workflows as Directed Acyclic Graphs (DAGs) using Python.</p>\n        <h3>Why Use Airflow?</h3>\n        <ul>\n          <li><strong>Scalability:</strong> Supports distributed execution with executors like Celery, Kubernetes, and Local.</li>\n          <li><strong>Dynamic Pipelines:</strong> DAGs are defined as Python code, enabling dynamic task generation.</li>\n          <li><strong>Extensibility:</strong> Integrates with external systems via Hooks, Sensors, and Operators.</li>\n          <li><strong>Monitoring:</strong> Provides a Web UI for tracking progress, logs, and debugging.</li>\n        </ul>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/\">Official Airflow Documentation</a></li>\n          <li><a href=\"https://github.com/apache/airflow\">Airflow GitHub Repository</a></li>\n        </ul>\n      "
  },
  {
    "title": "Installation and Setup",
    "content": "\n        <h3>Installing Airflow</h3>\n        <p>To install Airflow, use pip:</p>\n        <div class='code-container'> <button id='copyButton' class='copy-button' onclick='copyCode()'>Copy</button><pre><code class='language-bash'>pip install apache-airflow</code></pre></div>   \n        <h3>Initializing Airflow</h3>\n        <p>After installation, initialize the database and start the web server:</p>\n        <div class='code-container'> <button id='copyButton' class='copy-button' onclick='copyCode()'>Copy</button><pre><code class='language-bash'>\n  airflow db init\n  airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com\n  airflow webserver --port 8080\n  airflow scheduler\n        </code></pre></div>\n        <h3>Verify Installation</h3>\n        <p>Access the Airflow Web UI at <code>http://localhost:8080</code> and log in with the credentials created above.</p>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/start.html\">Airflow Quick Start Guide</a></li>\n        </ul>\n      "
  },
  {
    "title": "Airflow Basics (Components - DAGs, Operators, Tasks)",
    "content": "\n        <h3>DAGs (Directed Acyclic Graphs)</h3>\n        <p>A DAG defines the workflow structure with tasks and dependencies. Example:</p>\n        <div class='code-container'> <button id='copyButton' class='copy-button' onclick='copyCode()'>Copy</button><pre><code class='language-python'>\n  from airflow import DAG\n  from airflow.operators.dummy import DummyOperator\n  from datetime import datetime\n  \n  with DAG('example_dag', start_date=datetime(2025, 3, 1)) as dag:\n      start = DummyOperator(task_id='start')\n      end = DummyOperator(task_id='end')\n      start >> end\n        </code></pre></div>\n        <h3>Operators</h3>\n        <p>Operators define individual tasks. Common operators include:</p>\n        <ul>\n          <li><strong>BashOperator:</strong> Executes bash commands.</li>\n          <li><strong>PythonOperator:</strong> Runs Python functions.</li>\n          <li><strong>EmailOperator:</strong> Sends emails.</li>\n        </ul>\n        <h3>Tasks</h3>\n        <p>Tasks are instances of operators. They are executed by the Airflow scheduler based on the DAG's schedule.</p>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html\">DAG Concepts</a></li>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/concepts/operators.html\">Operator Concepts</a></li>\n        </ul>\n      "
  },
  {
    "title": "Default Imports in Airflow",
    "content": "\n          <h3>Commonly Used Imports</h3>\n          <p>These are the most frequently used imports when working with Apache Airflow:</p>\n          <div class='code-container'> <button id='copyButton' class='copy-button' onclick='copyCode()'>Copy</button><pre><code class='language-python'>\n    from airflow import DAG\n    from airflow.operators.python import PythonOperator\n    from airflow.operators.bash import BashOperator\n    from airflow.operators.dummy import DummyOperator\n    from airflow.sensors.filesystem import FileSensor\n    from airflow.providers.postgres.operators.postgres import PostgresOperator\n    from airflow.utils.dates import days_ago\n    from datetime import datetime, timedelta\n          </code></pre></div>\n          <h3>Explanation of Imports</h3>\n          <ul>\n            <li><strong>DAG:</strong> The core class for defining workflows.</li>\n            <li><strong>PythonOperator:</strong> Executes Python functions as tasks.</li>\n            <li><strong>BashOperator:</strong> Executes bash commands as tasks.</li>\n            <li><strong>DummyOperator:</strong> Placeholder task with no action.</li>\n            <li><strong>FileSensor:</strong> Waits for a file to appear in a directory.</li>\n            <li><strong>PostgresOperator:</strong> Executes SQL queries on PostgreSQL databases.</li>\n            <li><strong>days_ago:</strong> Utility function for setting start dates relative to the current date.</li>\n          </ul>\n          <h3>References:</h3>\n          <ul>\n            <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/concepts/operators.html\">Airflow Operators Documentation</a></li>\n          </ul>\n        "
  },
  {
    "title": "Standard DAG Syntax",
    "content": "\n          <h3>Basic DAG Definition</h3>\n          <p>A DAG is defined using the <code>DAG</code> context manager. Here's an example:</p>\n          <div class='code-container'> <button id='copyButton' class='copy-button' onclick='copyCode()'>Copy</button><pre><code class='language-python'>\n    from airflow import DAG\n    from airflow.operators.bash import BashOperator\n    from airflow.operators.python import PythonOperator\n    from datetime import datetime\n    \n    # Default arguments for tasks\n    default_args = {\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=5)\n    }\n    \n    # Define the DAG\n    with DAG(\n        dag_id='example_dag',\n        default_args=default_args,\n        description='A simple example DAG',\n        schedule_interval='@daily',\n        start_date=datetime(2025, 3, 1),\n        catchup=False\n    ) as dag:\n        # Define tasks\n        task1 = BashOperator(\n            task_id='bash_task',\n            bash_command='echo Hello World!'\n        )\n    \n        def print_hello():\n            print(\"Hello from Python!\")\n    \n        task2 = PythonOperator(\n            task_id='python_task',\n            python_callable=print_hello\n        )\n    \n        # Define dependencies\n        task1 >> task2\n          </code></pre></div>\n          <h3>Key Components</h3>\n          <ul>\n            <li><strong>dag_id:</strong> Unique identifier for the DAG.</li>\n            <li><strong>default_args:</strong> Default parameters for tasks (e.g., retries, retry delay).</li>\n            <li><strong>description:</strong> A brief description of the DAG's purpose.</li>\n            <li><strong>schedule_interval:</strong> Defines how often the DAG runs (e.g., CRON expression or preset like <code>@daily</code>).</li>\n            <li><strong>start_date:</strong> The date from which the DAG starts running.</li>\n            <li><strong>catchup:</strong> If <code>False</code>, skips backfilling past runs.</li>\n          </ul>\n          <h3>Task Dependencies</h3>\n          <p>Dependencies between tasks can be defined using bitwise operators:</p>\n          <div class='code-container'> <button id='copyButton' class='copy-button' onclick='copyCode()'>Copy</button><pre><code class='language-python'>\n    task1 >> task2  # task1 runs before task2\n    task2 << task3  # task3 runs before task2\n    [task1, task2] >> task3  # task3 runs after both task1 and task2\n          </code></pre></div>\n          <h3>References:</h3>\n          <ul>\n            <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html\">Airflow Tutorial</a></li>\n          </ul>\n        "
  },{
    "title": "Understanding Default Arguments",
    "content": "\n          <h3>What Are Default Arguments?</h3>\n          <p>Default arguments apply to all tasks in a DAG unless overridden. Example:</p>\n          <div class='code-container'> <button id='copyButton' class='copy-button' onclick='copyCode()'>Copy</button><pre><code class='language-python'>\n    default_args = {\n        'owner': 'airflow',\n        'depends_on_past': False,\n        'email': ['admin@example.com'],\n        'email_on_failure': False,\n        'email_on_retry': False,\n        'retries': 3,\n        'retry_delay': timedelta(minutes=5)\n    }\n          </code></pre></div>\n          <h3>Common Parameters</h3>\n          <ul>\n            <li><strong>owner:</strong> The owner of the DAG (usually a username).</li>\n            <li><strong>depends_on_past:</strong> If <code>True</code>, a task depends on the success of its previous run.</li>\n            <li><strong>email:</strong> Email addresses to notify on failure or retry.</li>\n            <li><strong>retries:</strong> Number of times to retry a failed task.</li>\n            <li><strong>retry_delay:</strong> Time to wait before retrying a failed task.</li>\n          </ul>\n          <h3>Overriding Defaults</h3>\n          <p>Individual tasks can override default arguments:</p>\n          <div class='code-container'> <button id='copyButton' class='copy-button' onclick='copyCode()'>Copy</button><pre><code class='language-python'>\n    task1 = BashOperator(\n        task_id='bash_task',\n        bash_command='echo Hello World!',\n        retries=5  # Overrides the default retries value\n    )\n          </code></pre></div>\n          <h3>References:</h3>\n          <ul>\n            <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html#default-arguments\">Default Arguments Documentation</a></li>\n          </ul>\n        "
  },{
    "title": "Scheduling with CRON Expressions",
    "content": "\n          <h3>CRON Expression Basics</h3>\n          <p>Airflow uses CRON expressions to define schedules. Format:</p>\n          <div class='code-container'> <button id='copyButton' class='copy-button' onclick='copyCode()'>Copy</button><pre><code class='language-bash'>\n    * * * * *\n    | | | | |\n    | | | | +--- Day of the week (0 - 6) (Sunday=0)\n    | | | +----- Month (1 - 12)\n    | | +------- Day of the month (1 - 31)\n    | +--------- Hour (0 - 23)\n    +----------- Minute (0 - 59)\n          </code></pre></div>\n          <h3>Examples</h3>\n          <ul>\n            <li><code>0 0 * * *</code>: Daily at midnight.</li>\n            <li><code>0 12 * * 1-5</code>: Weekdays at noon.</li>\n            <li><code>*/15 * * * *</code>: Every 15 minutes.</li>\n          </ul>\n          <h3>Using Presets</h3>\n          <p>Airflow provides preset schedules:</p>\n          <ul>\n            <li><code>@hourly</code>: Runs once per hour.</li>\n            <li><code>@daily</code>: Runs once per day.</li>\n            <li><code>@weekly</code>: Runs once per week.</li>\n          </ul>\n          <h3>References:</h3>\n          <ul>\n            <li><a href=\"https://crontab.guru/\">Cron Expression Generator</a></li>\n            <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/scheduler.html\">Scheduler Documentation</a></li>\n          </ul>\n        "
  },{
    "title": "Catchup and Backfilling",
    "content": "\n          <h3>What is Catchup?</h3>\n          <p>Catchup determines whether Airflow backfills missed runs between the <code>start_date</code> and the current date.</p>\n          <h3>Disabling Catchup</h3>\n          <p>To prevent backfilling, set <code>catchup=False</code> in the DAG definition:</p>\n          <div class='code-container'> <button id='copyButton' class='copy-button' onclick='copyCode()'>Copy</button><pre><code class='language-python'>\n    with DAG(\n        dag_id='example_dag',\n        start_date=datetime(2025, 3, 1),\n        schedule_interval='@daily',\n        catchup=False\n    ) as dag:\n        pass\n          </code></pre></div>\n          <h3>Enabling Catchup</h3>\n          <p>If <code>catchup=True</code>, Airflow will execute all missed runs. Use this for historical data processing.</p>\n          <h3>References:</h3>\n          <ul>\n            <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html#catchup\">Catchup Documentation</a></li>\n          </ul>\n        "
  },{
    "title": "TaskFlow API: Simplifying DAGs",
    "content": "\n          <h3>What is the TaskFlow API?</h3>\n          <p>The TaskFlow API simplifies DAG definitions by reducing boilerplate code. Example:</p>\n          <div class='code-container'> <button id='copyButton' class='copy-button' onclick='copyCode()'>Copy</button><pre><code class='language-python'>\n    from airflow.decorators import dag, task\n    from datetime import datetime\n    \n    @dag(\n        dag_id='taskflow_api_example',\n        start_date=datetime(2025, 3, 1),\n        schedule_interval='@daily'\n    )\n    def example_dag():\n        @task\n        def extract():\n            return {'data': [1, 2, 3]}\n    \n        @task\n        def transform(data):\n            return {'transformed_data': [x * 2 for x in data['data']]}\n    \n        @task\n        def load(transformed_data):\n            print(f\"Loaded: {transformed_data}\")\n    \n        data = extract()\n        transformed_data = transform(data)\n        load(transformed_data)\n    \n    example_dag_instance = example_dag()\n          </code></pre></div>\n          <h3>Advantages of TaskFlow API</h3>\n          <ul>\n            <li>Less boilerplate code.</li>\n            <li>Automatic dependency inference.</li>\n            <li>Improved readability.</li>\n          </ul>\n          <h3>References:</h3>\n          <ul>\n            <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html\">TaskFlow API Documentation</a></li>\n          </ul>\n        "
  },
  {
    "title": "Writing Your First DAG (Simple DAG Creation)",
    "content": "\n        <h3>Creating a Simple DAG</h3>\n        <p>Define a DAG with two tasks:</p>\n        <div class='code-container'> <button id='copyButton' class='copy-button' onclick='copyCode()'>Copy</button><pre><code class='language-python'>\n  from airflow import DAG\n  from airflow.operators.bash import BashOperator\n  from airflow.operators.python import PythonOperator\n  from datetime import datetime\n  \n  def print_hello():\n      print(\"Hello from Python!\")\n  \n  with DAG('first_dag', start_date=datetime(2025, 3, 1), schedule_interval='@daily') as dag:\n      bash_task = BashOperator(task_id='bash_task', bash_command='echo Hello World!')\n      python_task = PythonOperator(task_id='python_task', python_callable=print_hello)\n      bash_task >> python_task\n        </code></pre></div>\n        <h3>Explanation:</h3>\n        <ul>\n          <li><strong>DAG Context:</strong> Defined using the <code>with DAG()</code> context manager.</li>\n          <li><strong>Tasks:</strong> <code>BashOperator</code> and <code>PythonOperator</code>.</li>\n          <li><strong>Dependencies:</strong> Defined using <code>task1 >> task2</code>.</li>\n        </ul>\n      "
  },
  {
    "title": "Scheduling DAGs (CRON Expressions, Timetables)",
    "content": "\n        <h3>Scheduling with CRON</h3>\n        <p>Use CRON expressions to define schedules. Example:</p>\n        <div class='code-container'> <button id='copyButton' class='copy-button' onclick='copyCode()'>Copy</button><pre><code class='language-python'>\n  schedule_interval='0 0 * * *'  # Daily at midnight\n        </code></pre></div>\n        <h3>Timetables</h3>\n        <p>Custom timetables allow more complex scheduling logic. Example:</p>\n        <div class='code-container'> <button id='copyButton' class='copy-button' onclick='copyCode()'>Copy</button><pre><code class='language-python'>\n  from airflow.timetables.base import Timetable\n  \n  class CustomTimetable(Timetable):\n      def next_dagrun_info(self, last_automated_data_interval, restriction):\n          # Define custom logic here\n          pass\n        </code></pre></div>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://crontab.guru/\">Cron Expression Generator</a></li>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/concepts/timetable.html\">Timetables Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Task Dependencies (Upstream, Downstream, etc.)",
    "content": "\n        <h3>Defining Task Dependencies</h3>\n        <p>Tasks can be linked using bitwise operators or methods:</p>\n        <pre>\n  task1 >> task2  # task1 runs before task2\n  task2 << task3  # task3 runs before task2\n  task1.set_downstream(task2)  # Equivalent to task1 >> task2\n  task2.set_upstream(task1)    # Equivalent to task1 >> task2\n        </pre>\n        <h3>Complex Dependencies</h3>\n        <p>Use lists for parallel tasks:</p>\n        <pre>\n  [task1, task2] >> task3\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html#dependencies\">Dependency Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Dynamic DAG Generation",
    "content": "\n        <h3>Dynamic DAGs</h3>\n        <p>Create DAGs dynamically based on external data:</p>\n        <pre>\n  from airflow import DAG\n  from airflow.operators.dummy import DummyOperator\n  from datetime import datetime\n  \n  dag_ids = ['dynamic_dag_1', 'dynamic_dag_2']\n  \n  for dag_id in dag_ids:\n      with DAG(dag_id, start_date=datetime(2025, 3, 1)) as dag:\n          start = DummyOperator(task_id='start')\n          end = DummyOperator(task_id='end')\n          start >> end\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html#dynamic-dags\">Dynamic DAGs Tutorial</a></li>\n        </ul>\n      "
  },
  {
    "title": "Airflow CLI (Basic Commands)",
    "content": "\n        <h3>Common CLI Commands</h3>\n        <ul>\n          <li><code>airflow db init</code>: Initialize the metadata database.</li>\n          <li><code>airflow dags list</code>: List all DAGs.</li>\n          <li><code>airflow tasks test <dag_id> <task_id> <execution_date></code>: Test a specific task.</li>\n          <li><code>airflow webserver</code>: Start the Airflow Web UI.</li>\n          <li><code>airflow scheduler</code>: Start the Airflow scheduler.</li>\n        </ul>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html\">CLI Reference</a></li>\n        </ul>\n      "
  },
  {
    "title": "Airflow Web UI (Monitoring, Triggering, Debugging)",
    "content": "\n        <h3>Using the Web UI</h3>\n        <p>The Airflow Web UI provides tools for:</p>\n        <ul>\n          <li>Monitoring DAG runs and task statuses.</li>\n          <li>Triggering DAGs manually.</li>\n          <li>Viewing logs and debugging failed tasks.</li>\n        </ul>\n        <h3>Key Features</h3>\n        <ul>\n          <li><strong>Graph View:</strong> Visualize task dependencies.</li>\n          <li><strong>Tree View:</strong> Track task status over time.</li>\n          <li><strong>Gantt Chart:</strong> Analyze task durations.</li>\n        </ul>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/ui.html\">Web UI Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Configurations and Best Practices",
    "content": "\n        <h3>Configuration File</h3>\n        <p>Airflow configurations are stored in <code>airflow.cfg</code>. Key settings include:</p>\n        <ul>\n          <li><code>core.sql_alchemy_conn</code>: Database connection string.</li>\n          <li><code>webserver.web_server_port</code>: Port for the Web UI.</li>\n          <li><code>scheduler.max_threads</code>: Number of threads for the scheduler.</li>\n        </ul>\n        <h3>Best Practices</h3>\n        <ul>\n          <li>Use <code>catchup=False</code> to prevent backfilling unless needed.</li>\n          <li>Keep DAGs idempotent for safe re-runs.</li>\n          <li>Use environment variables for sensitive data.</li>\n        </ul>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html\">Configuration Reference</a></li>\n        </ul>\n      "
  },
  {
    "title": "Sensors and Hooks (External Systems Interaction)",
    "content": "\n        <h3>Sensors</h3>\n        <p>Sensors wait for a condition to be met. Example:</p>\n        <pre>\n  from airflow.sensors.filesystem import FileSensor\n  \n  file_sensor = FileSensor(\n      task_id='file_sensor',\n      filepath='/path/to/file',\n      timeout=600\n  )\n        </pre>\n        <h3>Hooks</h3>\n        <p>Hooks interact with external systems. Example:</p>\n        <pre>\n  from airflow.providers.postgres.hooks.postgres import PostgresHook\n  \n  hook = PostgresHook(postgres_conn_id='my_postgres_conn')\n  result = hook.get_records('SELECT * FROM my_table')\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/concepts/sensors.html\">Sensors Documentation</a></li>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.html\">Providers Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Custom Operators (Creating Your Own Operators)",
    "content": "\n        <h3>Creating a Custom Operator</h3>\n        <p>Define a custom operator by subclassing <code>BaseOperator</code>:</p>\n        <pre>\n  from airflow.models import BaseOperator\n  from airflow.utils.decorators import apply_defaults\n  \n  class MyCustomOperator(BaseOperator):\n      @apply_defaults\n      def __init__(self, my_param, *args, **kwargs):\n          super().__init__(*args, **kwargs)\n          self.my_param = my_param\n  \n      def execute(self, context):\n          print(f\"Running with param: {self.my_param}\")\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.html\">Custom Operator Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "XComs (Inter-task Communication)",
    "content": "\n        <h3>Using XComs</h3>\n        <p>XComs allow tasks to exchange small amounts of data:</p>\n        <pre>\n  from airflow import DAG\n  from airflow.operators.python import PythonOperator\n  from datetime import datetime\n  \n  def push_function(**kwargs):\n      kwargs['ti'].xcom_push(key='my_key', value='Hello, XCom!')\n  \n  def pull_function(**kwargs):\n      value = kwargs['ti'].xcom_pull(key='my_key')\n      print(f\"Pulled value: {value}\")\n  \n  with DAG('xcom_example', start_date=datetime(2025, 3, 1)) as dag:\n      push_task = PythonOperator(task_id='push_task', python_callable=push_function)\n      pull_task = PythonOperator(task_id='pull_task', python_callable=pull_function)\n      push_task >> pull_task\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html\">XComs Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Branching and SubDAGs (Conditional Execution)",
    "content": "\n        <h3>Branching</h3>\n        <p>Use the <code>BranchPythonOperator</code> to implement conditional logic:</p>\n        <pre>\n  from airflow import DAG\n  from airflow.operators.python import BranchPythonOperator\n  from airflow.operators.dummy import DummyOperator\n  from datetime import datetime\n  \n  def decide_branch():\n      return 'branch_a'\n  \n  with DAG('branching_example', start_date=datetime(2025, 3, 1)) as dag:\n      branch_task = BranchPythonOperator(task_id='branch_task', python_callable=decide_branch)\n      branch_a = DummyOperator(task_id='branch_a')\n      branch_b = DummyOperator(task_id='branch_b')\n      branch_task >> [branch_a, branch_b]\n        </pre>\n        <h3>SubDAGs</h3>\n        <p>SubDAGs group tasks into reusable components:</p>\n        <pre>\n  from airflow import DAG\n  from airflow.operators.subdag import SubDagOperator\n  from datetime import datetime\n  \n  def subdag(parent_dag_name, child_dag_name, args):\n      with DAG(f\"{parent_dag_name}.{child_dag_name}\", default_args=args) as dag:\n          DummyOperator(task_id='subdag_task')\n      return dag\n  \n  with DAG('subdag_example', start_date=datetime(2025, 3, 1)) as dag:\n      subdag_task = SubDagOperator(task_id='subdag_task', subdag=subdag('subdag_example', 'child_dag', {}))\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/concepts/branch.html\">Branching Documentation</a></li>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/concepts/subdags.html\">SubDAGs Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Integrating with AWS (S3, Glue, Redshift, etc.)",
    "content": "\n        <h3>AWS Integration</h3>\n        <p>Use Airflow's AWS provider package for integration:</p>\n        <pre>\n  from airflow.providers.amazon.aws.operators.s3 import S3ListOperator\n  from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\n  \n  list_files = S3ListOperator(\n      task_id='list_files',\n      bucket='my_bucket',\n      prefix='my_prefix'\n  )\n  \n  wait_for_file = S3KeySensor(\n      task_id='wait_for_file',\n      bucket_key='s3://my_bucket/my_file.txt',\n      timeout=600\n  )\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/index.html\">AWS Provider Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Integrating with Databases (PostgreSQL, MySQL)",
    "content": "\n        <h3>Database Integration</h3>\n        <p>Use database-specific operators and hooks:</p>\n        <pre>\n  from airflow.providers.postgres.operators.postgres import PostgresOperator\n  \n  create_table = PostgresOperator(\n      task_id='create_table',\n      postgres_conn_id='my_postgres_conn',\n      sql='''\n          CREATE TABLE IF NOT EXISTS my_table (\n              id SERIAL PRIMARY KEY,\n              name VARCHAR(255)\n          );\n      '''\n  )\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow-providers-postgres/stable/index.html\">PostgreSQL Provider Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "API Integrations (REST, GraphQL)",
    "content": "\n        <h3>REST API Integration</h3>\n        <p>Use the <code>SimpleHttpOperator</code> to call REST APIs:</p>\n        <pre>\n  from airflow.providers.http.operators.http import SimpleHttpOperator\n  \n  call_api = SimpleHttpOperator(\n      task_id='call_api',\n      http_conn_id='my_http_conn',\n      endpoint='/api/resource',\n      method='GET'\n  )\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow-providers-http/stable/index.html\">HTTP Provider Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Local Executor vs. Celery Executor",
    "content": "\n        <h3>Local Executor</h3>\n        <p>Runs tasks locally on a single machine. Suitable for development and testing.</p>\n        <h3>Celery Executor</h3>\n        <p>Distributes tasks across multiple worker nodes. Requires Redis or RabbitMQ as a message broker.</p>\n        <h3>Choosing an Executor</h3>\n        <ul>\n          <li><strong>LocalExecutor:</strong> For small-scale workflows.</li>\n          <li><strong>CeleryExecutor:</strong> For large-scale, distributed workflows.</li>\n        </ul>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/executor/local.html\">Local Executor Documentation</a></li>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/executor/celery.html\">Celery Executor Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Deployment Strategies (Docker, Kubernetes)",
    "content": "\n        <h3>Docker Deployment</h3>\n        <p>Use Docker Compose to deploy Airflow:</p>\n        <pre>\n  version: '3'\n  services:\n    postgres:\n      image: postgres:13\n      environment:\n        POSTGRES_USER: airflow\n        POSTGRES_PASSWORD: airflow\n        POSTGRES_DB: airflow\n    webserver:\n      image: apache/airflow:2.6.0\n      depends_on:\n        - postgres\n      environment:\n        AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n      ports:\n        - \"8080:8080\"\n      command: webserver\n        </pre>\n        <h3>Kubernetes Deployment</h3>\n        <p>Deploy Airflow on Kubernetes using Helm:</p>\n        <pre>\n  helm repo add apache-airflow https://airflow.apache.org\n  helm install airflow apache-airflow/airflow\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/helm-chart/stable/index.html\">Helm Chart Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Scaling Airflow (Worker Nodes, Scheduler Optimization)",
    "content": "\n        <h3>Scaling Workers</h3>\n        <p>Add more worker nodes to handle increased task loads.</p>\n        <h3>Optimizing the Scheduler</h3>\n        <p>Increase the number of scheduler threads:</p>\n        <pre>\n  [scheduler]\n  max_threads = 4\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/production-deployment.html\">Production Deployment Guide</a></li>\n        </ul>\n      "
  },
  {
    "title": "Writing Unit Tests for DAGs",
    "content": "\n        <h3>Testing DAGs</h3>\n        <p>Write unit tests to validate DAG definitions:</p>\n        <pre>\n  import unittest\n  from airflow.models import DagBag\n  \n  class TestDagIntegrity(unittest.TestCase):\n      def setUp(self):\n          self.dagbag = DagBag()\n  \n      def test_dag_loaded(self):\n          dag = self.dagbag.get_dag(dag_id='example_dag')\n          self.assertDictEqual({}, self.dagbag.import_errors)\n          self.assertIsNotNone(dag)\n  \n  if __name__ == '__main__':\n      unittest.main()\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/testing.html\">Testing Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Debugging DAGs and Tasks",
    "content": "\n        <h3>Debugging Tips</h3>\n        <ul>\n          <li>Check task logs in the Web UI for errors.</li>\n          <li>Use <code>airflow tasks test</code> to run tasks locally.</li>\n          <li>Enable verbose logging in <code>airflow.cfg</code>.</li>\n        </ul>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/logging-tasks.html\">Logging Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Logging and Monitoring Best Practices",
    "content": "\n        <h3>Best Practices</h3>\n        <ul>\n          <li>Centralize logs using tools like ELK Stack or CloudWatch.</li>\n          <li>Monitor DAG health with metrics like task success rate.</li>\n          <li>Set up alerts for failed tasks.</li>\n        </ul>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/index.html\">Monitoring Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Role-Based Access Control (RBAC)",
    "content": "\n        <h3>RBAC Configuration</h3>\n        <p>Enable RBAC in <code>airflow.cfg</code>:</p>\n        <pre>\n  [webserver]\n  rbac = True\n        </pre>\n        <h3>Managing Roles</h3>\n        <p>Create roles and assign permissions via the Web UI.</p>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/security/access-control.html\">RBAC Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "API Authentication",
    "content": "\n        <h3>Securing the API</h3>\n        <p>Enable API authentication in <code>airflow.cfg</code>:</p>\n        <pre>\n  [api]\n  auth_backend = airflow.api.auth.backend.basic_auth\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/security/api.html\">API Authentication Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "DAG Optimization Techniques",
    "content": "\n        <h3>Optimization Tips</h3>\n        <ul>\n          <li>Minimize the number of tasks in a DAG.</li>\n          <li>Use parallelism to reduce execution time.</li>\n          <li>Avoid long-running tasks by breaking them into smaller tasks.</li>\n        </ul>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html\">Best Practices Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Parallelism and Concurrency Settings",
    "content": "\n        <h3>Settings</h3>\n        <p>Configure parallelism and concurrency in <code>airflow.cfg</code>:</p>\n        <pre>\n  [core]\n  parallelism = 32\n  dag_concurrency = 16\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html\">Configuration Reference</a></li>\n        </ul>\n      "
  },
  {
    "title": "Improving Task Performance",
    "content": "\n        <h3>Tips for Performance</h3>\n        <ul>\n          <li>Use lightweight containers for tasks.</li>\n          <li>Cache intermediate results when possible.</li>\n          <li>Profile tasks to identify bottlenecks.</li>\n        </ul>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/performance-profiling.html\">Performance Profiling</a></li>\n        </ul>\n      "
  },
  {
    "title": "End-to-End Pipeline Example",
    "content": "\n        <h3>Example Pipeline</h3>\n        <p>An ETL pipeline that extracts data from a CSV, transforms it, and loads it into a database:</p>\n        <pre>\n  from airflow import DAG\n  from airflow.operators.python import PythonOperator\n  from airflow.providers.postgres.operators.postgres import PostgresOperator\n  from datetime import datetime\n  \n  def extract_data():\n      # Simulate extracting data\n      return [{'id': 1, 'name': 'Alice'}, {'id': 2, 'name': 'Bob'}]\n  \n  def transform_data(data):\n      # Transform data\n      return [{**row, 'name': row['name'].upper()} for row in data]\n  \n  with DAG('etl_pipeline', start_date=datetime(2025, 3, 1)) as dag:\n      create_table = PostgresOperator(\n          task_id='create_table',\n          postgres_conn_id='my_postgres_conn',\n          sql='''\n              CREATE TABLE IF NOT EXISTS users (\n                  id INT PRIMARY KEY,\n                  name VARCHAR(255)\n              );\n          '''\n      )\n  \n      extract = PythonOperator(task_id='extract', python_callable=extract_data)\n      transform = PythonOperator(task_id='transform', python_callable=lambda ti: transform_data(ti.xcom_pull(task_ids='extract')))\n      load = PostgresOperator(\n          task_id='load',\n          postgres_conn_id='my_postgres_conn',\n          sql=\"INSERT INTO users (id, name) VALUES (%s, %s);\",\n          parameters=[(row['id'], row['name']) for row in transform.output]\n      )\n  \n      create_table >> extract >> transform >> load\n        </pre>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/tutorial_etl.html\">ETL Tutorial</a></li>\n        </ul>\n      "
  },
  {
    "title": "Real-World Use Cases (ETL Pipelines, Data Lakes, etc.)",
    "content": "\n        <h3>ETL Pipelines</h3>\n        <p>Use Airflow to automate ETL workflows for data warehousing.</p>\n        <h3>Data Lakes</h3>\n        <p>Orchestrate data ingestion, transformation, and storage in data lakes.</p>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/use-cases.html\">Use Cases Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Common Issues and Troubleshooting",
    "content": "\n        <h3>Common Issues</h3>\n        <ul>\n          <li><strong>Backfilling:</strong> Disable with <code>catchup=False</code>.</li>\n          <li><strong>Task Failures:</strong> Check logs and retry logic.</li>\n          <li><strong>Deadlocks:</strong> Ensure proper resource allocation.</li>\n        </ul>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html\">Troubleshooting Guide</a></li>\n        </ul>\n      "
  },
  {
    "title": "Best Practices Summary",
    "content": "\n        <h3>Summary</h3>\n        <ul>\n          <li>Use modular DAGs for better readability.</li>\n          <li>Test DAGs thoroughly before deployment.</li>\n          <li>Monitor performance and optimize as needed.</li>\n        </ul>\n        <h3>References:</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html\">Best Practices Documentation</a></li>\n        </ul>\n      "
  },
  {
    "title": "Further Learning Resources",
    "content": "\n        <h3>Resources</h3>\n        <ul>\n          <li><a href=\"https://airflow.apache.org/docs/\">Official Airflow Documentation</a></li>\n          <li><a href=\"https://www.astronomer.io/guides/\">Astronomer Guides</a></li>\n          <li><a href=\"https://www.udemy.com/course/apache-airflow-course/\">Udemy Course on Airflow</a></li>\n        </ul>\n      "
  }
  ]


